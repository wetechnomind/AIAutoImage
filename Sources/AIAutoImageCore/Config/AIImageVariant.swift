//
//  AIImageVariant.swift
//  AIAutoImageCore
//
//  AI-powered image variant selection.
//
//  Features:
//   • Vision-based saliency analysis
//   • Face detection for clarity prioritization
//   • Scene semantics via VNClassifyImageRequest
//   • Optional CoreML-based variant picker model (“AIVariantPicker_v1”)
//   • Heuristic fallbacks for reliable results on all devices
//

import Foundation
import CoreGraphics
import UIKit
import Vision
import CoreML

// MARK: - Variant Enum
// ----------------------------------------------------------------------

/// Represents different size variants generated by the AIAutoImage pipeline.
///
/// Variants are used to:
/// - Choose the best resize scale
/// - Pick the optimal cached version
/// - Decide which version to load first (progressive loading)
///
/// Some variants have predefined sizes; `.custom` allows arbitrary dimensions.
public enum AIImageVariant: Sendable, Equatable {

    /// Very small thumbnail (≈64×64)
    case thumb

    /// Small preview (≈200×200)
    case small

    /// Medium size (≈400×400)
    case medium

    /// Large preview (≈800×800)
    case large

    /// Full-resolution image (no downscale)
    case full

    /// Custom user-defined variant.
    /// - Parameters:
    ///   - size: Target pixel dimensions.
    ///   - tag: Unique identifier for caching.
    case custom(size: CGSize, tag: String)

    /// Suggested pixel size for this variant.
    /// Returns `nil` for `.full` since it represents original resolution.
    public var pixelSize: CGSize? {
        switch self {
        case .thumb:  return CGSize(width: 64, height: 64)
        case .small:  return CGSize(width: 200, height: 200)
        case .medium: return CGSize(width: 400, height: 400)
        case .large:  return CGSize(width: 800, height: 800)
        case .full:   return nil
        case .custom(let size, _): return size
        }
    }

    /// String tag used for caching and variant identification.
    public var tag: String {
        switch self {
        case .thumb: return "thumb"
        case .small: return "small"
        case .medium: return "medium"
        case .large: return "large"
        case .full: return "full"
        case .custom(_, let tag): return tag
        }
    }
}


// MARK: - AI Variant Selector
// ----------------------------------------------------------------------

/// Automatically determines the best image variant using:
/// - CoreML model (“AIVariantPicker_v1”) if available
/// - Vision saliency scoring
/// - Face detection (portraits receive high-quality variants)
/// - Scene classification (landscapes → full, icons → small)
/// - Adaptive saliency-based heuristics
///
/// This system ensures the **most meaningful size** is chosen for each image.
public enum AIImageVariantSelector {

    // MARK: Main Selection Logic
    // --------------------------------------------------------------

    /**
     Returns the most appropriate variant for a given image.

     The decision pipeline:
      1. Try CoreML model “AIVariantPicker_v1”
      2. Vision saliency scoring
      3. Vision face detection
      4. Scene classification
      5. Heuristic range-based fallback

     - Parameter image: The source image.
     - Returns: Best matching `AIImageVariant`.
     */
    public static func bestVariant(for image: UIImage) async -> AIImageVariant {

        // --- Stage 1: ML model prediction ---
        if let mlVariant = await predictWithML(image) {
            return mlVariant
        }

        // --- Stage 2: Vision-based signals ---
        let saliency = await computeSaliency(image)
        let hasFace = await detectFace(image)
        let category = await classifyImage(image)

        // ----------------------------------------------------
        // AI HEURISTICS
        // ----------------------------------------------------

        // Portraits require higher clarity
        if hasFace {
            return saliency > 0.55 ? .full : .large
        }

        // Scene semantics
        if category.contains("landscape") || category.contains("outdoor") {
            return .full
        }
        if category.contains("icon") || category.contains("logo") {
            return .small
        }

        // Saliency ranges → variant mapping
        switch saliency {
        case 0.00...0.20: return .thumb
        case 0.20...0.35: return .small
        case 0.35...0.55: return .medium
        case 0.55...0.75: return .large
        default:          return .full
        }
    }


    // MARK: - ML Predictor
    // --------------------------------------------------------------

    /**
     Attempts variant prediction using CoreML model “AIVariantPicker_v1”.

     Expected output format:
     ```json
     { "variant": "medium" }
     ```

     - Returns: Variant if model exists + prediction succeeds.
     */
    private static func predictWithML(_ image: UIImage) async -> AIImageVariant? {

        guard let model = await AIModelManager.shared.model(named: "AIVariantPicker_v1"),
              let result = try? model.predict(inputs: ["image": image]),
              let variantString = result["variant"] as? String else {
            return nil
        }

        switch variantString.lowercased() {
        case "thumb":  return .thumb
        case "small":  return .small
        case "medium": return .medium
        case "large":  return .large
        case "full":   return .full
        default:       return nil
        }
    }


    // MARK: - Vision: Saliency
    // --------------------------------------------------------------

    /**
     Computes saliency score using Vision’s attention-based request.

     - Returns:
       Normalized range **0.0 – 1.0** representing visual importance.
     */
    private static func computeSaliency(_ image: UIImage) async -> Double {
        guard let cg = image.cgImage else { return 0.5 }

        if #available(iOS 15.0, *) {
            let req = VNGenerateAttentionBasedSaliencyImageRequest()
            let handler = VNImageRequestHandler(cgImage: cg)

            try? handler.perform([req])

            guard let obs = req.results?.first as? VNSaliencyImageObservation else {
                return 0.5
            }

            // Compute average intensity of attention heatmap
            let pb = obs.pixelBuffer
            CVPixelBufferLockBaseAddress(pb, .readOnly)
            defer { CVPixelBufferUnlockBaseAddress(pb, .readOnly) }

            let w = CVPixelBufferGetWidth(pb)
            let h = CVPixelBufferGetHeight(pb)
            let base = CVPixelBufferGetBaseAddress(pb)!.assumingMemoryBound(to: UInt8.self)
            let stride = CVPixelBufferGetBytesPerRow(pb)

            var sum: Double = 0
            for y in 0..<h {
                let row = base + y * stride
                for x in 0..<w {
                    sum += Double(row[x]) / 255.0
                }
            }

            return min(max(sum / Double(w * h), 0), 1)
        }

        return 0.5
    }


    // MARK: - Vision: Face Detection
    // --------------------------------------------------------------

    /**
     Detects faces using Vision.

     - Returns: `true` if any face rectangles are detected.
     */
    private static func detectFace(_ image: UIImage) async -> Bool {
        guard let cg = image.cgImage else { return false }

        let req = VNDetectFaceRectanglesRequest()
        let handler = VNImageRequestHandler(cgImage: cg, options: [:])

        try? handler.perform([req])
        return (req.results?.count ?? 0) > 0
    }


    // MARK: - Vision: Scene Classification
    // --------------------------------------------------------------

    /**
     Classifies the image using Vision’s on-device classifier.

     Useful for detecting scenes like:
     - landscape
     - outdoor
     - icon/logo

     - Returns: Lowercased identifier string or empty string.
     */
    private static func classifyImage(_ image: UIImage) async -> String {
        guard let cg = image.cgImage else { return "" }

        let req = VNClassifyImageRequest()
        let handler = VNImageRequestHandler(cgImage: cg, options: [:])

        try? handler.perform([req])
        return req.results?.first?.identifier.lowercased() ?? ""
    }
}
